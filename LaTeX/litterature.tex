\section{Litterature and contribution}\label{litterature}

Big Data is a collective term for massive data sets that consist of large, more varied and complex structures, see \textcite{zikopoulos2011}. These types of data are usually characterized by three properties - the three v's - \textit{variety}, \textit{velocity} and \textit{volume}. \textit{Variety} refers to all the different types of structures on the data, e.g. newspaper text, see \textcite{thorsrud2016}, and twitter feeds, see \textcite{antenucci2014}. \textit{Velocity} describes the gathering process as close to real time and \textit{volume} refers to the high dimensionality of the data available.

Big Data becomes relevant in the field of economics due to the lack of "hard" economic data available in real-time. Spesifically, one type of big data, SVIs, may be relevant for nowcasting, both due to the \textit{velocity} of the data which is near real-time, and because of the large \textit{volume} of dissagregated series. As pointed out in \textcite{wu2015}, the SVIs may reveal valuable information about the individualâ€™s intentions to make an economic transaction\footnote{The economic transaction can e.g. be related to buying goods, or it can be related to the employment situation.}. If this is the fact, these revealed intentions may be used to predict economic aggregates. 

My study relates to several attempts to use internet search data for prediction, in various fields. A famous example outside the field of economics is \textcite{ginsberg2009}, who used search data to predict the incidence of influenza-like diseases. Within the field of economics there has been much focus on variables like unemployment, retail sales, consumption and house prices\footnote{According to \textcite{choi2012} the first attempt to use internet search data for prediction in economics was \textcite{ettredge2005}.}. The reason is twofold. Firstly, these variables are all important for the state of the economy and linked to important variables like GDP and inflation. Secondly, they are typically reported on a monthly frequency. Since Google Trends starts in 2004, it is hard to evaluate predictive performance of variables that are measured at a lower frequency. This paper relates most closely to \textcite{anvik2010}\footnote{As far as I know, \textcite{anvik2010} is the only study on the use of SVIs for nowcasting the Norwegian economy.}, in terms of the research question and area of application. They use 19 different queries, aggregated into 4 categories motivated by search theory, to nowcast Norwegian unemployment. One of their main findings is that adding SVIs to an ARIMA model of the unemployment improves the prediction accuracy by up to 18 pct. over twelve months. However, due to a short sample\footnote{The sample used in \textcite{anvik2010} was only approximately 6 years.}, these results may be driven by a few large events, e.g. the fact that their pseudo-out-of-sample exercise started in June 2009, right after the Norwegian unemployment rate had been through a substantial negative shock during the financial crisis, see Figures \ref{rsales_sa} and \ref{rsales_d_sa}. Fortunately, we now have a longer sample of data available, and hence I am able to assess the stability of the performance of the SVI models during a longer period of time. Further, I improve upon the analysis in \textcite{anvik2010} by using a dynamic variable selection method to choose the top predictors throughout the sample, and therefore allow the nowcasting model to be more flexible over time.

There are several other examples of papers studying the use of SVIs to predict economic variables, and I will briefly present some of them. Perhaps the most famous example is \textcite{choi2012}. A particularly interesting finding in this paper is that some SVIs seem to help in identifying some of the turning points in initial claims for unemployment. \textcite{carriere2013} create an index consisting of 9 SVIs, specifically queries containing 9 different car manufacturers, to nowcast automobile sales in Chile. They find that the models that incorporated the constructed index outperformed both the in-sample and out-of-sample fit relative to simple benchmark models, specifically ARMA models. \textcite{vosen2012} use principal components analysis on a set of SVIs and use the extracted factors as predictors to nowcast consumption in Germany. They find that the models that include the SVI factors improve the out-of-sample nowcasts relative to survey-based indicators like the consumer confidence indicator and the retail trade confidence indicator, although not all the differences were statistically significant. \textcite{da2015} use online dictionaries to obtain a large set of daily SVIs, and perform backward-rolling regressions to pick the top predictors that they use to construct an index measuring investor sentiment. \textcite{boe2011} demonstrate the interest in SVIs from the perspective of a central bank, here represented by the Bank of England. They use SVIs related to unemployment and housing to nowcast unemployment and house prices in the UK. In an out-of-sample exercise they show that an SVI representing one single query (for each target variable) outperforms an AR(2) model in nowcasting unemployment and house prices, respectively, over a period of 31 months. It should be noted that they use the whole sample to choose the SVIs, including the evaluation sample on 31 periods.

In terms of methodology, there are several approaches for variable selection. Some widely used examples are GETS, see \textcite{campos2003}, forward stepwise regression and the Least Absolute Shrinkage and Selection Operator (LASSO), see \textcite{tibshirani1996}. LAR has several advantages. Firstly, contrary to GETS, LAR is designed to handle the case where the number of variables exceed the number of observations, because the full unrestricted model is never estimated. Secondly, LAR does not involve any testing, and thus avoids the challenge of inflated type-1 errors. Thirdly, the LAR is less "greedy" than e.g. forward stepwise regression, because it does not project all the covariates on the response, and hence does not force the residuals to be orthogonal to the predictors. Fourthly, contrary to the LASSO, the LAR algorithm is easier in a time series context because it does not require the specification of any tuning parameters, which are usually chosen by cross validation.